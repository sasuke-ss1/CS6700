{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S04Sn3SGiYwa",
        "outputId": "c7bda534-6e38-4edc-ec88-9113ad8addc6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MAX_EPISODES = 1500\n",
        "MAX_TIMESTEPS = 200\n",
        "\n",
        "alpha_policy = 1e-3\n",
        "alpha_value = 1e-4\n",
        "gamma = 0.99\n",
        "\n",
        "def softmax(logits):\n",
        "    max_logit = torch.max(logits)\n",
        "    shifted_logits = logits - max_logit\n",
        "    exp_shifted_logits = torch.exp(shifted_logits)\n",
        "    softmax_probs = exp_shifted_logits / torch.sum(exp_shifted_logits)\n",
        "    \n",
        "    return softmax_probs\n",
        "\n",
        "\n",
        "device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8fbeINDJigfi"
      },
      "outputs": [],
      "source": [
        "class reinforce(nn.Module):\n",
        "\n",
        "    def __init__(self, baseline = True):\n",
        "        super(reinforce, self).__init__()\n",
        "        # policy network\n",
        "        self.fc1 = nn.Linear(4, 128)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 2)\n",
        "\n",
        "\n",
        "        self.baseline = baseline\n",
        "\n",
        "        #Value network\n",
        "        self.v1 = nn.Linear(4, 128)\n",
        "        self.v2 = nn.Linear(128, 128)\n",
        "        self.v3 = nn.Linear(128, 1)\n",
        "\n",
        "        self.params_policy = [\n",
        "          {\"params\": self.fc1.parameters()},  {\"params\": self.fc2.parameters()}, {\"params\": self.fc3.parameters()}\n",
        "        ]\n",
        "\n",
        "        self.params_value = [\n",
        "          {\"params\": self.v1.parameters()},  {\"params\": self.v2.parameters()}, {\"params\": self.v3.parameters()}\n",
        "        ]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.tanh(x)\n",
        "        # x = self.fc2(x)\n",
        "        # x = self.tanh(x)\n",
        "        x = self.fc3(x)\n",
        "        x = softmax(x)\n",
        "        return x # [B, 2]\n",
        "\n",
        "    def value_forward(self,x):\n",
        "        x = self.v1(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.v2(x)\n",
        "        # x = self.relu(x)\n",
        "        x = self.v3(x)\n",
        "        return x # [B, 1]\n",
        "\n",
        "    def get_action(self, state):\n",
        "        with torch.no_grad():\n",
        "          state = torch.tensor(state).to(device) # [4,]\n",
        "          state = torch.unsqueeze(state, 0) # [1, 4]\n",
        "          \n",
        "          probs = self.forward(state) # [1, 2]\n",
        "          probs = torch.squeeze(probs, 0) # [2,]\n",
        "          \n",
        "          if math.isnan(probs[0]) or math.isnan(probs[1]):\n",
        "            print(probs)\n",
        "            print(state)\n",
        "            print(self.fc1(state))\n",
        "      \n",
        "          action = torch.multinomial(probs, 1, replacement=True)\n",
        "\n",
        "        return int(action.item())\n",
        "\n",
        "    def pi(self, s, a):\n",
        "        s = torch.tensor([s]).to(device)\n",
        "        probs = self.forward(s)\n",
        "        probs = torch.squeeze(probs, 0)\n",
        "        \n",
        "        return probs[a]\n",
        "\n",
        "    def v(self,s):\n",
        "        s = torch.tensor([s]).to(device)\n",
        "        state_value = self.value_forward(s)\n",
        "\n",
        "        return state_value\n",
        "\n",
        "    def update_weight(self, states, actions, rewards, optimizer, optimizer_value = None):\n",
        "        G = torch.tensor([0]).to(device)\n",
        "        # for each step of the episode t = T - 1, ..., 0\n",
        "        # r_tt represents r_{t+1}\n",
        "        if self.baseline == False:\n",
        "          for s_t, a_t, r_tt in zip(states[::-1], actions[::-1], rewards[::-1]):\n",
        "              G = torch.tensor([r_tt]).to(device) + gamma * G\n",
        "              loss = (-1.0) * G * torch.log(self.pi(s_t, a_t))\n",
        "              # update policy parameter \\theta\n",
        "\n",
        "              optimizer.zero_grad()\n",
        "              loss.backward()\n",
        "              optimizer.step()\n",
        "        \n",
        "        else:\n",
        "          for idx, (s_t, a_t, r_tt) in enumerate(zip(states, actions, rewards)):\n",
        "              G = 0\n",
        "              for t in range(idx, len(rewards)):\n",
        "                G += rewards[t] + gamma * G\n",
        "                \n",
        "              delta =  gamma**idx * (G - self.v(s_t).detach())\n",
        "              print(G)\n",
        "              loss = (-1.0) * delta * torch.log(self.pi(s_t, a_t))\n",
        "              \n",
        "              y = r_tt + gamma * self.v(states[idx + 1]) if idx < len(states)-1 else r_tt + gamma * torch.zeros([1, 1])\n",
        "              \n",
        "              val_loss = F.mse_loss(self.v(s_t), y.detach())#-0.5*(G-self.v(s_t))**2\n",
        "              # update policy and value parameters \\theta and \\w\n",
        "              optimizer.zero_grad()\n",
        "              optimizer_value.zero_grad()\n",
        "              loss.backward()\n",
        "              val_loss.backward()\n",
        "              torch.nn.utils.clip_grad_norm_(self.parameters(), max_norm=0.5)\n",
        "              optimizer.step()\n",
        "              optimizer_value.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "nNv3mcGiijrs",
        "outputId": "f1e40988-9596-46df-9b1e-5813211d0d84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/sasuke/miniconda3/envs/pytorch/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
            "  logger.warn(\n",
            "/home/sasuke/miniconda3/envs/pytorch/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/home/sasuke/miniconda3/envs/pytorch/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n",
            "/home/sasuke/miniconda3/envs/pytorch/lib/python3.9/site-packages/gym/envs/classic_control/cartpole.py:168: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 finished after 18 timesteps\n",
            "Episode 0\tAverage Score: nan\n",
            "241945.95607968204\n",
            "241945.95607968204\n",
            "121580.37993953872\n",
            "61095.16579876317\n",
            "30700.585828524207\n",
            "15426.927552022215\n",
            "7751.722387950862\n",
            "3894.835370829579\n",
            "1956.7011913716478\n",
            "982.7644177746973\n",
            "493.3489536556268\n",
            "247.41153450031499\n",
            "123.824891708701\n",
            "61.7210511099\n",
            "30.513091010000004\n",
            "14.830699000000003\n",
            "6.950100000000001\n",
            "2.99\n",
            "5.956346741303047e+59\n",
            "5.956346741303047e+59\n",
            "2.9931382771312594e+59\n",
            "1.5040887976479497e+59\n",
            "7.558227275558329e+58\n",
            "3.7980962700702996e+58\n",
            "1.9085832019858714e+58\n",
            "9.590791476720845e+57\n",
            "4.8194143193651415e+57\n",
            "2.421737355869816e+57\n",
            "1.2168745601435177e+57\n",
            "6.1141586882377196e+56\n",
            "3.0716527017063357e+56\n",
            "1.542755221037292e+56\n",
            "7.744650297463192e+55\n",
            "3.8838955673577734e+55\n",
            "1.9438178135359656e+55\n",
            "9.689043694044008e+54\n",
            "4.7899811607195954e+54\n",
            "2.3281406917123375e+54\n",
            "1.0910349283921074e+54\n",
            "4.693737407940035e+53\n",
            "1.5698118421204148e+53\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "0.0\n",
            "tensor([nan, nan])\n",
            "tensor([[-0.0227,  0.0250, -0.0415, -0.0314]])\n",
            "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
            "         nan, nan, nan, nan, nan, nan, nan, nan]])\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "invalid multinomial distribution (encountering probability entry < 0)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[24], line 60\u001b[0m\n\u001b[1;32m     55\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m     env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m---> 60\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[24], line 23\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m return_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m timesteps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(MAX_TIMESTEPS):\n\u001b[0;32m---> 23\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[1;32m     25\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(action)\n",
            "Cell \u001b[0;32mIn[23], line 58\u001b[0m, in \u001b[0;36mreinforce.get_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mprint\u001b[39m(state)\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(state))\n\u001b[0;32m---> 58\u001b[0m   action \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(action\u001b[38;5;241m.\u001b[39mitem())\n",
            "\u001b[0;31mRuntimeError\u001b[0m: invalid multinomial distribution (encountering probability entry < 0)"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "\n",
        "    env = gym.make('CartPole-v1')\n",
        "\n",
        "    agent = reinforce().to(device)\n",
        "    optimizer = optim.Adam(agent.params_policy, lr=alpha_policy)\n",
        "    optimizer_value = optim.Adam(agent.params_value, lr=alpha_value)\n",
        "\n",
        "    avg_scores = []\n",
        "    episodic_returns = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "\n",
        "    for i_episode in range(MAX_EPISODES):\n",
        "\n",
        "        state = env.reset()\n",
        "\n",
        "        states = []\n",
        "        actions = []\n",
        "        rewards = [0]\n",
        "        return_episode = 0\n",
        "\n",
        "        for timesteps in range(MAX_TIMESTEPS):\n",
        "            action = agent.get_action(state)\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            rewards.append(reward)\n",
        "\n",
        "            # return_episode = reward + gamma * return_episode\n",
        "            return_episode = reward + 1 * return_episode\n",
        "\n",
        "            if done and i_episode % 100 == 0:\n",
        "                print(\"Episode {} finished after {} timesteps\".format(i_episode, timesteps+1))\n",
        "                break\n",
        "\n",
        "        if i_episode % 100 == 0:\n",
        "                print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=195.0:\n",
        "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "            break\n",
        "\n",
        "        episodic_returns.append(return_episode)\n",
        "\n",
        "        scores_window.append(return_episode)\n",
        "        avg_scores.append(np.mean(scores_window))\n",
        "        if agent.baseline == False:\n",
        "          agent.update_weight(states, actions, rewards, optimizer)\n",
        "        else:\n",
        "          agent.update_weight(states, actions, rewards, optimizer, optimizer_value)\n",
        "\n",
        "    plt.plot(avg_scores, label = 'Running average')\n",
        "    plt.plot(episodic_returns, label = 'Epsiodic returns')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Reward')\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
